# ğŸ“š Local GPT Assistant (RAG-based Mini Project)

### ğŸš€ Overview
A **local AI assistant** that lets users upload documents (TXT, PDF, DOCX, CSV), indexes them using embeddings (SentenceTransformers + FAISS), and answers context-based questions â€” 100% offline and OpenAI-free.

If no answer is found in the data, it responds:
> â€œI donâ€™t have enough information in the uploaded documents.â€

### ğŸ§  Features
- Upload multiple files (`.pdf`, `.txt`, `.csv`, `.docx`)
- Extracts and chunks text (~300â€“500 words each)
- Embeds chunks with SentenceTransformers (`all-MiniLM-L6-v2`)
- Stores and retrieves context using FAISS
- Generates answers locally using a HuggingFace model (`distilgpt2`)
- No hallucinations â€” answers strictly from user-provided documents
- Streamlit-based interactive interface

### ğŸ› ï¸ Tech Stack
- **Language:** Python 3.10+
- **Libraries:** Streamlit, FAISS, SentenceTransformers, Transformers, PyTorch, NumPy, Pandas, PyPDF, python-docx
- **Framework:** Streamlit UI
- **Storage:** Local FAISS vector index

### âš™ï¸ Setup Instructions

```bash
# 1ï¸âƒ£ Clone the repo
git clone https://github.com/Nisha8817/Local-GPT-Assistant.git
cd Local-GPT-Assistant

# 2ï¸âƒ£ Create and activate virtual environment
python -m venv .venv
.venv\Scripts\activate  # On Windows

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 4ï¸âƒ£ Run Streamlit app
streamlit run app.py
